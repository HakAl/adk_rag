# Fully Local Agent with Google ADK and Ollama

This project demonstrates a Retrieval-Augmented Generation (RAG) agent built with the Google Agent Development Kit (ADK). The entire workflow, from data ingestion to conversational inference, runs entirely on a local machine using Ollama for serving large language models and ChromaDB as a vector store.

This serves as a practical example of an offline-capable setup.

## Core Technologies

*   **Google Agent Development Kit (ADK):** The core framework for defining the agent's behavior, instructions, and tool usage.
*   **Ollama:** A tool for running open-source large language models (like Llama 3) locally. It serves both the embedding model for ingestion and the chat model for generation.
*   **LangChain:** A framework used to streamline the data ingestion pipeline, including document loading, text splitting, and interfacing with embedding models and vector stores.
*   **ChromaDB:** A local, file-based vector database used to store the document embeddings for efficient retrieval.

## Project Structure

```
/
├── data/
│   └── your_document_1.pdf         # Place your source PDFs here
│   └── your_document_2.pdf
├── chroma_db/
│   └── ...                         # Local vector database (generated by ingest.py)
├── ingest.py                       # Script to process PDFs and create the vector store
├── agent.py                        # The main script to run the conversational agent
└── requirements.txt                # Python dependencies
```

## 🚀 Setup and Installation

Follow these steps to get the project running.

### 1. Install Ollama
First, ensure you have [Ollama](https://ollama.com/) installed and running on your machine.

### 2. Clone the Repository
Clone this project to your local machine.
```bash
git clone <your-repo-url>
cd <your-repo-directory>
```

### 3. Create a Python Virtual Environment
It's highly recommended to use a virtual environment to manage dependencies.
```bash
# Create the virtual environment
python -m venv venv

# Activate it (on macOS/Linux)
source venv/bin/activate

# Or on Windows
.\venv\Scripts\activate
```

### 4. Install Python Dependencies

```
google-generativeai
google-cloud-aiplatform
litellm
langchain
langchain-community
langchain-chroma
pypdf
sentence-transformers
```

```bash
pip install -r requirements.txt```

### 5. Download Local LLMs with Ollama
This project requires two types of models: one for generating text embeddings and another for conversational chat and reasoning.

Open your terminal and pull the models using Ollama:
```bash
# An efficient model for creating embeddings
ollama pull nomic-embed-text

# A capable model for chat and tool use
ollama pull llama3.1:8b-instruct-q4_K_M
```
*You can substitute `llama3.1:8b-instruct-q4_K_M` with another capable model if you prefer, but be sure to update the model name in `agent.py`.*

## ⚙️ How to Run

The process is divided into two stages: first, you ingest your data, and second, you run the interactive agent.

### Step 1: Ingest Your Documents
This step reads your PDF files, splits them into chunks, generates embeddings, and stores them in the local ChromaDB vector store.

1.  Place all the PDF files you want to use as a knowledge base into the `data/` directory.
2.  Run the ingestion script from your terminal:
    ```bash
    python ingest.py
    ```

You only need to run this script once, or whenever you add, remove, or change the documents in the `data/` directory.

### Step 2: Chat with Your Agent
This will start the interactive command-line interface where you can ask questions. The agent will use the knowledge stored in your local vector database to answer.

1.  Run the agent script:
    ```bash
    python agent.py
    ```
2.  The script will load the model and you will see a prompt.
    ```
    Using model: llama3.1:8b-instruct-q4_K_M

    ❓>
    ```
3.  Ask your questions. The agent will retrieve relevant information and generate an answer with sources.
4.  To exit the application, type `exit` or `quit` and press Enter.

## 🔧 How It Works

*   **`ingest.py`:** The ingestion pipeline uses LangChain to:
    1.  **Load:** Read all PDF files from the `data/` directory.
    2.  **Split:** Break the documents into small, overlapping text chunks.
    3.  **Embed:** Use the `nomic-embed-text` model via Ollama to convert each chunk into a numerical vector (embedding).
    4.  **Store:** Save these embeddings and their corresponding text content into a local ChromaDB database located in the `chroma_db/` directory.

*   **`agent.py`:** The conversational agent:
    1.  **Initializes** a Google ADK `Agent` configured to use your local Ollama chat model (`llama3.1:8b-instruct-q4_K_M`).
    2.  The agent is provided with a single tool: `rag_query`.
    3.  When you ask a question, the ADK framework determines that the `rag_query` tool must be called.
    4.  The `rag_query` function takes your question, converts it into an embedding, and searches the ChromaDB for the most similar text chunks.
    5.  These chunks (the context) are combined with your original question into a new prompt, which is then sent to the local `llama3.1:8b-instruct-q4_K_M` model to generate a final, synthesized answer.