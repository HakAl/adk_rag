# Application Configuration
ENVIRONMENT=development
DEBUG=false

# Provider Configuration
# Choose: 'ollama' or 'llamacpp'
PROVIDER_TYPE=ollama

# Ollama Configuration (when PROVIDER_TYPE=ollama)
OLLAMA_BASE_URL=http://localhost:11434
EMBEDDING_MODEL=nomic-embed-text
CHAT_MODEL=phi3:mini

# llama.cpp Configuration (when PROVIDER_TYPE=llamacpp)
# Paths to GGUF model files (relative to /app/models in container)
LLAMACPP_EMBEDDING_MODEL_PATH=/app/models/embeddings/nomic-embed-text-q4_k_m.gguf
LLAMACPP_CHAT_MODEL_PATH=/app/models/chat/phi3-mini-4k-instruct-q4_k_m.gguf
LLAMACPP_N_CTX=2048
LLAMACPP_N_BATCH=512
LLAMACPP_N_THREADS=
LLAMACPP_TEMPERATURE=0.7
LLAMACPP_MAX_TOKENS=512

# Vector Store Performance Settings
# Number of documents to retrieve per query (lower = faster)
RETRIEVAL_K=3

# ChromaDB HNSW Index Settings (lower values = faster queries, slightly lower accuracy)
CHROMA_HNSW_CONSTRUCTION_EF=100
CHROMA_HNSW_SEARCH_EF=50

# Logging
LOG_LEVEL=INFO
LOG_TO_FILE=false

# Multi-Provider Configuration (optional)
# Leave blank to use only local provider

# Anthropic Configuration
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-3-5-haiku-latest

# Google Configuration
GOOGLE_API_KEY=
GOOGLE_MODEL=gemini-2.0-flash