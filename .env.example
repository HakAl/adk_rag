# Application Configuration
ENVIRONMENT=development
DEBUG=false

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================
# PostgreSQL connection URL
# Format: postgresql+asyncpg://username:password@host:port/database
DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/rag_agent
DATABASE_ECHO=false

# ============================================================================
# CLOUD PROVIDERS (PRIMARY - Fast & Smart)
# ============================================================================
# Anthropic Configuration (RECOMMENDED - Best performance)
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-sonnet-4-20250514

# Google Configuration (Alternative cloud provider)
GOOGLE_API_KEY=
GOOGLE_MODEL=gemini-2.0-flash-exp

# Cloud Fallback Configuration
CLOUD_RETRY_ATTEMPTS=3
ENABLE_LOCAL_FALLBACK=true
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5
CIRCUIT_BREAKER_TIMEOUT=60

# ============================================================================
# LOCAL PROVIDER (FALLBACK - Reliable but slower)
# ============================================================================
# Provider Configuration
# Choose: 'ollama' or 'llamacpp'
PROVIDER_TYPE=llamacpp

# Ollama Configuration (when PROVIDER_TYPE=ollama)
OLLAMA_BASE_URL=http://localhost:11434
EMBEDDING_MODEL=nomic-embed-text
CHAT_MODEL=phi3:mini

# Models Base Directory
MODELS_BASE_DIR=/path/to/models

# llama.cpp Configuration (when PROVIDER_TYPE=llamacpp)
# Paths to GGUF model files (relative to MODELS_BASE_DIR)
LLAMACPP_EMBEDDING_MODEL_PATH=embeddings/nomic-embed-text-v1.5-q4_k_m.gguf
LLAMACPP_CHAT_MODEL_PATH=chat/phi3-mini-4k-instruct-q4_k_m.gguf
LLAMACPP_N_CTX=2048
LLAMACPP_N_BATCH=512
LLAMACPP_N_THREADS=8
LLAMACPP_TEMPERATURE=0.7
LLAMACPP_MAX_TOKENS=512

# llama-server configuration
LLAMA_SERVER_PATH=/path/to/llama-server
LLAMA_SERVER_PORT=8080
LLAMA_SERVER_HOST=127.0.0.1
LLAMA_SERVER_MISTRAL_PORT=8081

# ============================================================================
# VECTOR STORE PERFORMANCE SETTINGS
# ============================================================================
# Number of documents to retrieve per query (lower = faster)
RETRIEVAL_K=3

# ChromaDB HNSW Index Settings (lower values = faster queries, slightly lower accuracy)
CHROMA_HNSW_CONSTRUCTION_EF=100
CHROMA_HNSW_SEARCH_EF=50

# ============================================================================
# LOGGING
# ============================================================================
LOG_LEVEL=INFO
LOG_TO_FILE=false

# ============================================================================
# ROUTER CONFIGURATION
# ============================================================================
# Router classifies requests into: code_validation, rag_query, code_generation,
# code_analysis, complex_reasoning, general_chat
#
# Cloud providers (Anthropic/Google) are used automatically if API keys are set
# Local router is used as fallback if cloud unavailable
#
# Path to router model (relative to MODELS_BASE_DIR) - OPTIONAL
# Only needed if no cloud API keys available
# Example: ROUTER_MODEL_PATH=phi3-mini-4k-instruct-q4.gguf
ROUTER_MODEL_PATH=

# Router model context settings (for local router only)
ROUTER_N_CTX=2048
ROUTER_N_BATCH=512
ROUTER_N_THREADS=  # Leave empty to auto-detect

# Router generation settings
ROUTER_TEMPERATURE=0.1  # Lower for more deterministic routing decisions
ROUTER_MAX_TOKENS=256   # Routing decisions are short

# ============================================================================
# COORDINATOR AGENT CONFIGURATION
# ============================================================================
# Enable coordinator agent with router-based specialist delegation
# When enabled, /chat/coordinator endpoint uses router to classify requests
# and delegates to specialized agents
#
# SPECIALISTS PRIORITY:
# 1. Cloud (Anthropic) - Fast, smart, parallel execution (~600ms for 3 specialists)
# 2. Cloud (Google) - Fast, smart, fallback (~600ms for 3 specialists)
# 3. Local (Phi-3) - Slower, reliable fallback (~5-9s for 3 specialists)
#
# When disabled, /chat/coordinator falls back to regular single-agent behavior
USE_COORDINATOR_AGENT=false

# Note: Coordinator uses router classification to select specialists:
# - code_validation -> Code Validator (cloud/local)
# - rag_query -> Knowledge Assistant (cloud/local)
# - code_generation -> Code Generator (cloud/local)
# - code_analysis -> Code Analyst (cloud/local)
# - complex_reasoning -> Reasoning Specialist (cloud/local)
# - general_chat -> General Assistant (cloud/local)
# ============================================================================

# ============================================================================
# DEPLOYMENT SCENARIOS
# ============================================================================

# SCENARIO 1: Cloud-Only (Fastest, Recommended for Production)
# DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/rag_agent
# ANTHROPIC_API_KEY=sk-ant-...
# GOOGLE_API_KEY=...  # Optional backup
# USE_COORDINATOR_AGENT=true
# No local models needed!

# SCENARIO 2: Cloud with Local Fallback (Best of both worlds)
# DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/rag_agent
# ANTHROPIC_API_KEY=sk-ant-...
# LLAMACPP_CHAT_MODEL_PATH=chat/phi3-mini-4k-instruct-q4_k_m.gguf
# ENABLE_LOCAL_FALLBACK=true
# USE_COORDINATOR_AGENT=true

# SCENARIO 3: Local-Only (Offline, Privacy-focused)
# DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/rag_agent
# PROVIDER_TYPE=llamacpp
# LLAMACPP_CHAT_MODEL_PATH=chat/phi3-mini-4k-instruct-q4_k_m.gguf
# ROUTER_MODEL_PATH=phi3-mini-4k-instruct-q4.gguf
# USE_COORDINATOR_AGENT=true
# ============================================================================


# ============================================================================
# FRONTEND
# ============================================================================

# API Configuration
# Leave empty for development (uses Vite proxy)
# Set to your production API URL for production builds
VITE_API_URL=

# Example production configuration:
# VITE_API_URL=https://api.yourdomain.com

# App Version (optional)
VITE_VERSION=0.1.0