# Application Configuration
ENVIRONMENT=development
DEBUG=false

# Provider Configuration
# Choose: 'ollama' or 'llamacpp'
PROVIDER_TYPE=ollama

# Ollama Configuration (when PROVIDER_TYPE=ollama)
OLLAMA_BASE_URL=http://localhost:11434
EMBEDDING_MODEL=nomic-embed-text
CHAT_MODEL=phi3:mini
MODELS_BASE_DIR=/path/to/models

# llama.cpp Configuration (when PROVIDER_TYPE=llamacpp)
# Paths to GGUF model files (relative to MODELS_BASE_DIR)
LLAMACPP_EMBEDDING_MODEL_PATH=embeddings/nomic-embed-text-v1.5-q4_k_m.gguf
LLAMACPP_CHAT_MODEL_PATH=chat/phi3-mini-4k-instruct-q4_k_m.gguf
LLAMACPP_N_CTX=2048
LLAMACPP_N_BATCH=512
LLAMACPP_N_THREADS=8
LLAMACPP_TEMPERATURE=0.7
LLAMACPP_MAX_TOKENS=512

# llama-server configuration
LLAMA_SERVER_PATH=/path/to/llama-server
LLAMA_SERVER_PORT=8080
LLAMA_SERVER_HOST=127.0.0.1

# Vector Store Performance Settings
# Number of documents to retrieve per query (lower = faster)
RETRIEVAL_K=3

# ChromaDB HNSW Index Settings (lower values = faster queries, slightly lower accuracy)
CHROMA_HNSW_CONSTRUCTION_EF=100
CHROMA_HNSW_SEARCH_EF=50

# Logging
LOG_LEVEL=INFO
LOG_TO_FILE=false

# Multi-Provider Configuration (optional)
# Leave blank to use only local provider

# Anthropic Configuration
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-3-5-haiku-latest

# Google Configuration
GOOGLE_API_KEY=
GOOGLE_MODEL=gemini-2.0-flash

# ============================================================================
# ROUTER CONFIGURATION (REQUIRED FOR COORDINATOR)
# ============================================================================
# Router classifies requests into: code_validation, rag_query, code_generation,
# code_analysis, complex_reasoning, general_chat
#
# IMPORTANT: Router is REQUIRED when USE_COORDINATOR_AGENT=true
# The coordinator uses router classification to delegate to specialists
#
# Path to router model (relative to MODELS_BASE_DIR)
# Example: ROUTER_MODEL_PATH=phi3-mini-4k-instruct-q4.gguf
ROUTER_MODEL_PATH=

# Router model context settings
ROUTER_N_CTX=2048
ROUTER_N_BATCH=512
ROUTER_N_THREADS=  # Leave empty to auto-detect

# Router generation settings
ROUTER_TEMPERATURE=0.3  # Lower for more deterministic routing decisions
ROUTER_MAX_TOKENS=256   # Routing decisions are short

# Example configuration:
# MODELS_BASE_DIR=./models
# ROUTER_MODEL_PATH=phi3-mini-4k-instruct-q4.gguf


# ============================================================================
# COORDINATOR AGENT CONFIGURATION
# ============================================================================
# Enable coordinator agent with router-based specialist delegation
# When enabled, /chat/coordinator endpoint uses router to classify requests
# and delegates to specialized agents (code_validator, rag_specialist, etc.)
#
# REQUIRES: ROUTER_MODEL_PATH must be set (router is mandatory for coordinator)
#
# When disabled, /chat/coordinator falls back to regular single-agent behavior
USE_COORDINATOR_AGENT=false

# Note: Coordinator uses router classification to select specialists:
# - code_validation -> code_validator agent
# - rag_query -> rag_specialist agent
# - code_generation -> code_generator agent
# - code_analysis -> code_analyzer agent
# - complex_reasoning -> reasoning_specialist agent
# - general_chat -> general_assistant agent
# ============================================================================