# Application Configuration
ENVIRONMENT=development
DEBUG=false

# Provider Configuration
# Choose: 'ollama' or 'llamacpp'
PROVIDER_TYPE=ollama

# Ollama Configuration (when PROVIDER_TYPE=ollama)
OLLAMA_BASE_URL=http://localhost:11434
EMBEDDING_MODEL=nomic-embed-text
CHAT_MODEL=phi3:mini
MODELS_BASE_DIR=/path/to/models

# llama.cpp Configuration (when PROVIDER_TYPE=llamacpp)
# Paths to GGUF model files (relative to MODELS_BASE_DIR)
LLAMACPP_EMBEDDING_MODEL_PATH=embeddings/nomic-embed-text-v1.5-q4_k_m.gguf
LLAMACPP_CHAT_MODEL_PATH=chat/phi3-mini-4k-instruct-q4_k_m.gguf
LLAMACPP_N_CTX=2048
LLAMACPP_N_BATCH=512
LLAMACPP_N_THREADS=8
LLAMACPP_TEMPERATURE=0.7
LLAMACPP_MAX_TOKENS=512

# llama-server configuration
LLAMA_SERVER_PATH=/path/to/llama-server
LLAMA_SERVER_PORT=8080
LLAMA_SERVER_HOST=127.0.0.1

# Vector Store Performance Settings
# Number of documents to retrieve per query (lower = faster)
RETRIEVAL_K=3

# ChromaDB HNSW Index Settings (lower values = faster queries, slightly lower accuracy)
CHROMA_HNSW_CONSTRUCTION_EF=100
CHROMA_HNSW_SEARCH_EF=50

# Logging
LOG_LEVEL=INFO
LOG_TO_FILE=false

# Multi-Provider Configuration (optional)
# Leave blank to use only local provider

# Anthropic Configuration
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-3-5-haiku-latest

# Google Configuration
GOOGLE_API_KEY=
GOOGLE_MODEL=gemini-2.0-flash

# Router Configuration (Optional)
# If not set, router is disabled and requests go directly to main agent
# Defaults to llama.cpp for speed

# Path to router model (relative to MODELS_BASE_DIR)
# Example: ROUTER_MODEL_PATH=phi3-mini-4k-instruct-q4.gguf
ROUTER_MODEL_PATH=

# Router model context settings
ROUTER_N_CTX=2048
ROUTER_N_BATCH=512
ROUTER_N_THREADS=  # Leave empty to auto-detect

# Router generation settings
ROUTER_TEMPERATURE=0.3  # Lower for more deterministic routing decisions
ROUTER_MAX_TOKENS=256   # Routing decisions are short

# Example configuration:
# MODELS_BASE_DIR=./models
# ROUTER_MODEL_PATH=phi3-mini-4k-instruct-q4.gguf


# ============================================================================
# COORDINATOR AGENT CONFIGURATION (NEW)
# ============================================================================
# Enable coordinator agent with specialist delegation
# When enabled, /chat/coordinator endpoint routes to specialized agents
# When disabled, /chat/coordinator falls back to regular single-agent behavior
USE_COORDINATOR_AGENT=false

# Note: Coordinator requires the same model configuration as regular agent
# It uses phi3:mini by default for fast routing decisions
# Specialists use phi3:mini (fast tasks) or mistral (complex tasks)
# ============================================================================